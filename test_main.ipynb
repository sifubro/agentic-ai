{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daab1014-76cf-4904-91f3-1a85e10eef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agentic Orchestration Framework (from scratch)\n",
    "\n",
    "Features implemented:\n",
    "- Graph-based orchestration where each node is a Task executed by an Agent\n",
    "- Agents communicate via structured JSON Messages and JSON Responses\n",
    "- Session management with TTL\n",
    "- Memory management (short-term per-session + long-term per-user) with simple keyword recall\n",
    "- Authentication manager (user creation, login, token validation) with secure password hashing\n",
    "- Message parsing + validation (no external libraries, JSON-only interchange)\n",
    "- Simple in-memory persistence and optional disk write/read for long-term memory\n",
    "- Dynamic task creation support: agents can return `next_tasks` in JSON responses that the orchestrator will insert into the graph\n",
    "\n",
    "No external libraries used — only Python standard library.\n",
    "\n",
    "Example usage is at the bottom under `if __name__ == \"__main__\"` which demonstrates:\n",
    "- creating users and authenticating\n",
    "- creating agents and registering skills\n",
    "- building a simple two-node graph (fetch -> process)\n",
    "- running the orchestrator and printing the JSON outputs\n",
    "\n",
    "This is a foundational, educational implementation — not production hardened.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import secrets\n",
    "import hashlib\n",
    "import hmac\n",
    "import base64\n",
    "import threading\n",
    "import logging\n",
    "import uuid\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"AgenticOrchestrator\")\n",
    "\n",
    "# ---------- Utilities\n",
    "\n",
    "'''\n",
    "Walkthrough — file organization & utilities\n",
    "\n",
    "Top-level imports & logging\n",
    "\n",
    "Only standard library modules used: json, time, uuid, secrets, hashlib, hmac, threading, etc.\n",
    "\n",
    "Logging configured for visibility; useful during development.\n",
    "\n",
    "Utility functions\n",
    "\n",
    "now_ts() / iso_now() — unified timestamp helpers.\n",
    "\n",
    "generate_id(prefix) — short unique ids for nodes/sessions/users to keep things traceable.\n",
    "\n",
    "ensure_json_serializable(obj) — lightweight conversion helper to ensure stored results / memories are JSON-compatible (handles set, objects with __dict__, otherwise str() fallbacks). This avoids runtime serialization errors when storing in memory or returning results.\n",
    "\n",
    "Why: Small, deterministic helpers reduce duplication and help keep stored data consistent and serializable.\n",
    "'''\n",
    "\n",
    "def now_ts() -> float:\n",
    "    return time.time()\n",
    "\n",
    "\n",
    "def iso_now() -> str:\n",
    "    return datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "\n",
    "def generate_id(prefix: str = \"id\") -> str:\n",
    "    return f\"{prefix}_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "\n",
    "def ensure_json_serializable(obj: Any) -> Any:\n",
    "    \"\"\"Try to convert known non-serializable types to serializable ones.\n",
    "    This is lightweight — users should return simple JSON-friendly types.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json.dumps(obj)\n",
    "        return obj\n",
    "    except TypeError:\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            return {k: ensure_json_serializable(v) for k, v in obj.__dict__.items()}\n",
    "        return str(obj)\n",
    "\n",
    "\n",
    "# ---------- Authentication Manager\n",
    "\n",
    "class AuthError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class AuthManager:\n",
    "    '''\n",
    "    Purpose: create users, log in, and manage short-lived tokens.\n",
    "    **Key points:**\n",
    "    - Stores users in an in-memory dict: {username: {user_id, pw_hash, salt, created_at}}.\n",
    "    - Password hashing uses HMAC-SHA256(password, salt) stored as hex. Salt is random per user.\n",
    "    - Login issues a token (secrets.token_urlsafe(32)) with expiry now + token_ttl.\n",
    "    - validate_token(token) returns the user_id if valid, else None.\n",
    "    - revoke_token(token) allows invalidation.\n",
    "\n",
    "    **Design trade-offs / notes:**\n",
    "        - Simple, standard-library approach avoids external dependencies.\n",
    "        - This is not as resistant to offline brute-force or as feature-rich as PBKDF2/Argon2 — PBKDF2/Argon2 and salted iterations would be recommended for production.\n",
    "        - Tokens are stored in memory (no DB), so restarting the process loses active sessions/tokens.\n",
    "    '''\n",
    "    def __init__(self, token_ttl_seconds: int = 3600):\n",
    "        self._users: Dict[str, Dict[str, Any]] = {}  # username -> {user_id, pw_hash, salt, created_at}\n",
    "        self._tokens: Dict[str, Dict[str, Any]] = {}  # token -> {user_id, expires_at}\n",
    "        self._token_ttl = token_ttl_seconds\n",
    "        logger.info(\"AuthManager initialized (token ttl %ds)\", token_ttl_seconds)\n",
    "        print(\"AuthManager initialized (token ttl %ds)\", token_ttl_seconds\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash_password(password: str, salt: str) -> str:\n",
    "        # Use HMAC-SHA256 for password hashing with salt\n",
    "        return hmac.new(salt.encode(), password.encode(), hashlib.sha256).hexdigest()\n",
    "\n",
    "    def create_user(self, username: str, password: str) -> str:\n",
    "        if username in self._users:\n",
    "            raise AuthError(\"username_already_exists\")\n",
    "        user_id = generate_id(\"user\")\n",
    "        salt = secrets.token_hex(16)\n",
    "        pw_hash = self._hash_password(password, salt)\n",
    "        self._users[username] = {\n",
    "            \"user_id\": user_id,\n",
    "            \"pw_hash\": pw_hash,\n",
    "            \"salt\": salt,\n",
    "            \"created_at\": now_ts(),\n",
    "        }\n",
    "        logger.info(\"Created user %s (id=%s)\", username, user_id)\n",
    "        print(\"Created user %s (id=%s)\", username, user_id)\n",
    "        return user_id\n",
    "\n",
    "    def login(self, username: str, password: str) -> str:\n",
    "        if username not in self._users:\n",
    "            raise AuthError(\"invalid_credentials\")\n",
    "        record = self._users[username]\n",
    "        expected_hash = record[\"pw_hash\"]\n",
    "        salt = record[\"salt\"]\n",
    "        if not hmac.compare_digest(expected_hash, self._hash_password(password, salt)):\n",
    "            raise AuthError(\"invalid_credentials\")\n",
    "        token = secrets.token_urlsafe(32)\n",
    "        expires_at = now_ts() + self._token_ttl\n",
    "        self._tokens[token] = {\"user_id\": record[\"user_id\"], \"expires_at\": expires_at}\n",
    "        logger.info(\"User %s logged in (token=%s) expires=%s\", username, token, datetime.utcfromtimestamp(expires_at).isoformat())\n",
    "        return token\n",
    "\n",
    "    def validate_token(self, token: str) -> Optional[str]:\n",
    "        info = self._tokens.get(token)\n",
    "        if not info:\n",
    "            return None\n",
    "        if now_ts() > info[\"expires_at\"]:\n",
    "            del self._tokens[token]\n",
    "            return None\n",
    "        return info[\"user_id\"]\n",
    "\n",
    "    def revoke_token(self, token: str):\n",
    "        if token in self._tokens:\n",
    "            del self._tokens[token]\n",
    "\n",
    "\n",
    "# ---------- Session Management\n",
    "\n",
    "class SessionExpired(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SessionManager:\n",
    "    '''\n",
    "    Purpose: manage ephemeral sessions tied to users.\n",
    "    Key points:\n",
    "    - Each session is a dict {session_id, user_id, created_at, expires_at, metadata, graph}.\n",
    "    - create_session(user_id, metadata) creates a session and stores TTL.\n",
    "    - get_session(session_id) validates expiry and returns session; raises SessionExpired if missing/expired.\n",
    "    - touch_session extends TTL; end_session deletes it.\n",
    "\n",
    "    Why: Sessions provide scoping for short-term memory and graph execution. Session metadata is used for passing orchestration hints (e.g., which processor agent to use).\n",
    "    '''\n",
    "    def __init__(self, session_ttl_seconds: int = 3600):\n",
    "        self._sessions: Dict[str, Dict[str, Any]] = {}  # session_id -> {user_id, created_at, expires_at, metadata}\n",
    "        self._session_ttl = session_ttl_seconds\n",
    "        logger.info(\"SessionManager initialized (ttl %ds)\", session_ttl_seconds)\n",
    "        print(\"SessionManager initialized (ttl %ds)\", session_ttl_seconds)\n",
    "\n",
    "    def create_session(self, user_id: str, metadata: Optional[Dict[str, Any]] = None) -> str:\n",
    "        session_id = generate_id(\"sess\")\n",
    "        now = now_ts()\n",
    "        self._sessions[session_id] = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"created_at\": now,\n",
    "            \"expires_at\": now + self._session_ttl,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"graph\": None,  # orchestrator will attach graph\n",
    "        }\n",
    "        logger.info(\"Created session %s for user %s\", session_id, user_id)\n",
    "        print(\"Created session %s for user %s\", session_id, user_id)\n",
    "        return session_id\n",
    "\n",
    "    def get_session(self, session_id: str) -> Dict[str, Any]:\n",
    "        s = self._sessions.get(session_id)\n",
    "        if not s:\n",
    "            raise SessionExpired(\"session_not_found\")\n",
    "        if now_ts() > s[\"expires_at\"]:\n",
    "            del self._sessions[session_id]\n",
    "            raise SessionExpired(\"session_expired\")\n",
    "        return s\n",
    "\n",
    "    def touch_session(self, session_id: str):\n",
    "        s = self.get_session(session_id)\n",
    "        s[\"expires_at\"] = now_ts() + self._session_ttl\n",
    "\n",
    "    def end_session(self, session_id: str):\n",
    "        if session_id in self._sessions:\n",
    "            del self._sessions[session_id]\n",
    "\n",
    "\n",
    "# ---------- Memory Management\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"Simple memory manager:\n",
    "    - short-term memory: per-session circular buffer (max_items)\n",
    "    - long-term memory: per-user list persisted optionally to disk (simple JSON)\n",
    "    - recall: basic keyword matching in stored \"text\" fields\n",
    "\n",
    "    Memory manager (MemoryManager)\n",
    "    Purpose: two-tier memory: short-term (session-scoped) and long-term (user-scoped).\n",
    "    Implementation:\n",
    "    -Short-term: per-session deque(maxlen=short_term_max) for recent records.\n",
    "    -Long-term: per-user list appended; optional JSON file persistence if long_term_path provided.\n",
    "    - add_short(session_id, item) and add_long(user_id, item, persist=False).\n",
    "    - recall(user_id, keywords, session_id=None, limit=10) — naive keyword matching:\n",
    "        --serializes each memory entry with json.dumps(...).lower()\n",
    "        --returns matches containing any keyword (searches short-term first if session_id provided)\n",
    "\n",
    "    Why and tradeoffs:\n",
    "    - Keeps memory simple and easy to inspect.\n",
    "    - Keyword matching is fast and dependency-free but not semantically rich; if you want semantic recall use embeddings and a vector index.\n",
    "    - Persistence is simple JSON dump — fine for prototypes, not recommended for heavy production use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, short_term_max: int = 50, long_term_path: Optional[str] = None):\n",
    "        self._short_term: Dict[str, deque] = defaultdict(lambda: deque(maxlen=short_term_max))\n",
    "        self._long_term: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
    "        self._long_term_path = long_term_path\n",
    "        if long_term_path and os.path.exists(long_term_path):\n",
    "            try:\n",
    "                with open(long_term_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    self._long_term.update(data)\n",
    "                    logger.info(\"Loaded long-term memory from %s\", long_term_path)\n",
    "                    print(\"Loaded long-term memory from %s\", long_term_path)\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed loading long-term memory file; starting empty\")\n",
    "                print(\"Failed loading long-term memory file; starting empty\")\n",
    "\n",
    "    def add_short(self, session_id: str, item: Dict[str, Any]):\n",
    "        item_copy = ensure_json_serializable(item)\n",
    "        self._short_term[session_id].append({\"ts\": now_ts(), \"item\": item_copy})\n",
    "        logger.debug(\"Short-term memory added for %s: %s\", session_id, item_copy)\n",
    "        print()\n",
    "\n",
    "    def list_short(self, session_id: str) -> List[Dict[str, Any]]:\n",
    "        return list(self._short_term.get(session_id, []))\n",
    "\n",
    "    def add_long(self, user_id: str, item: Dict[str, Any], persist: bool = False):\n",
    "        item_copy = ensure_json_serializable(item)\n",
    "        self._long_term[user_id].append({\"ts\": now_ts(), \"item\": item_copy})\n",
    "        logger.debug(\"Long-term memory added for %s: %s\", user_id, item_copy)\n",
    "        print()\n",
    "        if persist and self._long_term_path:\n",
    "            try:\n",
    "                with open(self._long_term_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(self._long_term, f, indent=2)\n",
    "                logger.info(\"Persisted long-term memory to %s\", self._long_term_path)\n",
    "                print()\n",
    "            except Exception:\n",
    "                logger.exception(\"Failed to persist long-term memory\")\n",
    "                print()\n",
    "\n",
    "    def recall(self, user_id: str, keywords: List[str], session_id: Optional[str] = None, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Return items that contain any of the keywords in their string representation.\n",
    "        This is naive keyword matching but effective for small-scale demos.\n",
    "        \"\"\"\n",
    "        results: List[Tuple[float, Dict[str, Any]]] = []\n",
    "        def match_item(item):\n",
    "            '''\n",
    "            This function:\n",
    "            - Takes a single argument item.\n",
    "            - Converts that item (whatever data structure it is) into a lowercased string representation.\n",
    "            - Checks whether any of the words in a list called keywords appear in that string.\n",
    "            - Returns True if there’s at least one match, otherwise False.\n",
    "            '''\n",
    "\n",
    "            #Converts the item (which might be a dict, list, or other JSON-serializable object) into a JSON-formatted string \n",
    "            text = json.dumps(item, ensure_ascii=False).lower()\n",
    "            # This is a generator expression that loops through all kw in the list (or iterable) keywords.\n",
    "            # For each keyword kw: kw.lower() ensures the keyword is also lowercased.\n",
    "            # (kw.lower() in text) checks if the keyword appears anywhere in the string representation of item\n",
    "            # any(...) returns: True if at least one keyword is found in the text.  False if no keywords are found\n",
    "            return any(kw.lower() in text for kw in keywords)\n",
    "\n",
    "        # search short-term first (if session_id provided)\n",
    "        if session_id:\n",
    "            # Iterates through the deque from most recent to oldest (because we want the latest matches first)\n",
    "            # So this loop walks through the most recent records stored for that session\n",
    "            for rec in reversed(self._short_term.get(session_id, [])):\n",
    "                # Each rec is a dictionary. checks if that content contains any of the keywords (as we explained earlier).\n",
    "                # If it matches, we process it further.\n",
    "                if match_item(rec[\"item\"]):\n",
    "                    # [(1728383823, {\"message\": \"hello\"}),(1728383821, {\"message\": \"python\"}),]\n",
    "                    results.append((rec[\"ts\"], rec[\"item\"]))\n",
    "                    #If we’ve already collected enough matches (limit), we don’t need to keep searching.\n",
    "                    # This makes the search efficient, stopping early instead of scanning everything.\n",
    "                    if len(results) >= limit:\n",
    "                        # We return just the items, not the timestamps. r[1] extracts the second element from each (ts, item) tuple.\n",
    "                        return [r[1] for r in results]\n",
    "\n",
    "        # search long-term (user-level)\n",
    "        # self._long_term is a defaultdict(list) mapping:  user_id (string) ➝ a list of records.\n",
    "        for rec in reversed(self._long_term.get(user_id, [])):\n",
    "            if match_item(rec[\"item\"]):\n",
    "                results.append((rec[\"ts\"], rec[\"item\"]))\n",
    "                if len(results) >= limit:\n",
    "                    break\n",
    "        return [r[1] for r in results[:limit]]\n",
    "\n",
    "\n",
    "# ---------- Messaging / Parsing\n",
    "\n",
    "class MessageValidationError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Message:\n",
    "    '''\n",
    "    Message class\n",
    "    - Standardized message schema: required keys from, to, task_id, payload.\n",
    "    - Methods: to_dict(), to_json(), and parse(raw) accepts dict or JSON string.\n",
    "    - Timestamps are recorded (ts).\n",
    "    '''\n",
    "    REQUIRED_KEYS = {\"from\", \"to\", \"task_id\", \"payload\"}\n",
    "\n",
    "    def __init__(self, frm: str, to: str, task_id: str, payload: Dict[str, Any], metadata: Optional[Dict[str, Any]] = None):\n",
    "        self.frm = frm\n",
    "        self.to = to\n",
    "        self.task_id = task_id\n",
    "        self.payload = payload\n",
    "        self.metadata = metadata or {}\n",
    "        self.ts = now_ts()\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"from\": self.frm,\n",
    "            \"to\": self.to,\n",
    "            \"task_id\": self.task_id,\n",
    "            \"payload\": ensure_json_serializable(self.payload),\n",
    "            \"metadata\": ensure_json_serializable(self.metadata),\n",
    "            \"ts\": self.ts,\n",
    "        }\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        return json.dumps(self.to_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def parse(raw: Any) -> \"Message\":\n",
    "        # Accept dict or JSON string\n",
    "        if isinstance(raw, str):\n",
    "            try:\n",
    "                raw = json.loads(raw)\n",
    "            except json.JSONDecodeError:\n",
    "                raise MessageValidationError(\"invalid_json\")\n",
    "        if not isinstance(raw, dict):\n",
    "            raise MessageValidationError(\"message_must_be_object\")\n",
    "        missing = Message.REQUIRED_KEYS - set(raw.keys())\n",
    "        if missing:\n",
    "            raise MessageValidationError(f\"missing_keys: {sorted(list(missing))}\")\n",
    "        return Message(frm=str(raw[\"from\"]), to=str(raw[\"to\"]), task_id=str(raw[\"task_id\"]), payload=raw[\"payload\"], metadata=raw.get(\"metadata\"))\n",
    "\n",
    "\n",
    "def validate_agent_response(resp: Dict[str, Any]):\n",
    "    '''\n",
    "    Agent response validation\n",
    "    - validate_agent_response(resp) ensures response is an object with a status key (\"ok\" or \"error\").\n",
    "    - Optionally allows result, next_tasks, memories.\n",
    "    - next_tasks must be a list (if present).\n",
    "\n",
    "    Why: By forcing structured messages and responses we can orchestrate reliably between agents and detect malformed behavior early.\n",
    "    '''\n",
    "    # Basic validation for agent responses (must be JSON-serializable and contain `status` key)\n",
    "    if not isinstance(resp, dict):\n",
    "        raise MessageValidationError(\"response_must_be_object\")\n",
    "    if \"status\" not in resp:\n",
    "        raise MessageValidationError(\"response_missing_status\")\n",
    "    if resp[\"status\"] not in {\"ok\", \"error\"}:\n",
    "        raise MessageValidationError(\"response_status_invalid\")\n",
    "    # result is optional but if present must be serializable\n",
    "    if \"result\" in resp:\n",
    "        ensure_json_serializable(resp[\"result\"])\n",
    "    # next_tasks optional: list of node descriptors\n",
    "    if \"next_tasks\" in resp:\n",
    "        if not isinstance(resp[\"next_tasks\"], list):\n",
    "            raise MessageValidationError(\"next_tasks_must_be_list\")\n",
    "\n",
    "\n",
    "# ---------- Agent model\n",
    "\n",
    "class Agent:\n",
    "    '''\n",
    "    Purpose: encapsulate skills that can be invoked by name.\n",
    "\n",
    "    Key features:\n",
    "    - register_skill(name, func) — attach callable skill functions.\n",
    "    - handle_message(message, session, memory) — decides which skill to call (looks at message.metadata[skill] or payload[action] or falls back to 'default'), builds ctx and executes the skill.\n",
    "    - ctx contains agent identity, session, message metadata, memory manager reference, and timestamp.\n",
    "\n",
    "    Skill functions are expected to have signature:\n",
    "        def skill(payload: dict, ctx: dict) -> dict\n",
    "    and return validated JSON-like dicts, e.g.:\n",
    "        { \"status\":\"ok\", \"result\": {...}, \"memories\":[...], \"next_tasks\":[...] }\n",
    "    Why: This pattern gives clear separation: \n",
    "    - agents implement domain logic; \n",
    "    - orchestrator controls flow. \n",
    "    - ctx gives access to session/memory without coupling agents to orchestrator internals.\n",
    "    '''\n",
    "    def __init__(self, agent_id: str, name: str, role: str = \"assistant\"):\n",
    "        self.agent_id = agent_id\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self._skills: Dict[str, Callable[[Dict[str, Any], Dict[str, Any]], Dict[str, Any]]] = {}\n",
    "        logger.info(\"Agent created: %s (%s)\", name, agent_id)\n",
    "        print()\n",
    "\n",
    "    def register_skill(self, name: str, func: Callable[[Dict[str, Any], Dict[str, Any]], Dict[str, Any]]):\n",
    "        \"\"\"Register a skill function.\n",
    "        The function signature should be func(payload: dict, ctx: dict) -> dict (JSON-like response)\n",
    "        \"\"\"\n",
    "        self._skills[name] = func\n",
    "        logger.debug(\"Agent %s registered skill %s\", self.agent_id, name)\n",
    "        print()\n",
    "\n",
    "    def handle_message(self, message: Message, session: Dict[str, Any], memory: MemoryManager) -> Dict[str, Any]:\n",
    "        logger.info(\"Agent %s handling message %s -> %s\", self.agent_id, message.frm, message.to)\n",
    "        print()\n",
    "        # Decide which skill to call: first check metadata.skill then payload.action\n",
    "        skill = message.metadata.get(\"skill\") or message.payload.get(\"action\")\n",
    "        if not skill:\n",
    "            # fallback to default skill 'default'\n",
    "            skill = \"default\"\n",
    "        if skill not in self._skills:\n",
    "            logger.warning(\"Agent %s missing skill %s\", self.agent_id, skill)\n",
    "            print()\n",
    "            return {\"status\": \"error\", \"error\": f\"missing_skill:{skill}\"}\n",
    "        # Build context\n",
    "        ctx = {\n",
    "            \"agent\": {\"id\": self.agent_id, \"name\": self.name, \"role\": self.role},\n",
    "            \"session\": session,\n",
    "            \"message_meta\": message.metadata,\n",
    "            \"ts\": now_ts(),\n",
    "            \"memory\": memory,\n",
    "        }\n",
    "        try:\n",
    "            result = self._skills[skill](message.payload, ctx)\n",
    "            # Validate response\n",
    "            if not isinstance(result, dict):\n",
    "                return {\"status\": \"error\", \"error\": \"skill_returned_non_object\"}\n",
    "            validate_agent_response(result)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Agent skill %s raised\", skill)\n",
    "            print()\n",
    "            return {\"status\": \"error\", \"error\": f\"skill_exception:{str(e)}\"}\n",
    "\n",
    "\n",
    "# ---------- Task Node + Graph\n",
    "\n",
    "class TaskNode:\n",
    "    '''\n",
    "    Represents one unit of work in the graph.\n",
    "\n",
    "    Fields:\n",
    "    - node_id, name, agent_id, action (skill name), inputs (parent node ids), metadata.\n",
    "    - Runtime fields: status (pending, running, done, error), result, attempts.\n",
    "\n",
    "    .to_dict() returns JSON-friendly view.\n",
    "\n",
    "    Role in graph: nodes declare dependencies (inputs) and are executed when all parents are done.\n",
    "    '''\n",
    "    def __init__(self, node_id: str, name: str, agent_id: str, action: Optional[str] = None, inputs: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None):\n",
    "        self.node_id = node_id\n",
    "        self.name = name\n",
    "        self.agent_id = agent_id\n",
    "        self.action = action or \"default\"\n",
    "        self.inputs = inputs or []  # list of node_ids that are prerequisites\n",
    "        self.metadata = metadata or {}\n",
    "        self.status = \"pending\"  # pending, running, done, error\n",
    "        self.result: Optional[Dict[str, Any]] = None\n",
    "        self.attempts = 0\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"node_id\": self.node_id,\n",
    "            \"name\": self.name,\n",
    "            \"agent_id\": self.agent_id,\n",
    "            \"action\": self.action,\n",
    "            \"inputs\": self.inputs,\n",
    "            \"status\": self.status,\n",
    "            \"attempts\": self.attempts,\n",
    "            \"result\": ensure_json_serializable(self.result),\n",
    "        }\n",
    "\n",
    "\n",
    "class GraphOrchestrator:\n",
    "\n",
    "    \n",
    "    \"\"\"Orchestrates execution of TaskNodes connected as a directed acyclic graph (DAG).\n",
    "    - Nodes are added and edges are implied via node.inputs\n",
    "    - Agents are registered and executed synchronously\n",
    "    - Agents return JSON responses; orchestrator stores them and can add new nodes dynamically\n",
    "\n",
    "    Graph orchestrator (GraphOrchestrator)\n",
    "\n",
    "    This is the core of coordination. Main responsibilities:\n",
    "\n",
    "    1. Agent & node registration\n",
    "\n",
    "    - register_agent(agent), add_node(node) and add_edge(parent, child).\n",
    "    - Maintains _children and _parents adjacency maps for quick lookup.\n",
    "\n",
    "    2. Readiness detection\n",
    "\n",
    "    - _ready_nodes() iterates nodes and returns those pending whose parents are all done.\n",
    "    - This provides a simple DAG execution semantics.\n",
    "\n",
    "    3. Execution loop (execute_graph(session_id))\n",
    "\n",
    "    - Attaches graph snapshot to session.\n",
    "    - Loop:\n",
    "        - Find ready nodes.\n",
    "        - For each ready node:\n",
    "\n",
    "            - Mark running; increment attempts.\n",
    "            - Find agent (error if missing).\n",
    "            - Build node payload with parent results:\n",
    "            - payload = {\"action\": node.action, \"inputs\": { parent_id: parent.result }, \"metadata\": node.metadata}\n",
    "            - Wrap in a Message and call agent.handle_message(msg, session, memory).\n",
    "            - Validate response.\n",
    "            - On status == \"ok\" -> mark done and optionally store memories.\n",
    "            - On status != \"ok\" -> mark error.\n",
    "            - Handle next_tasks: agent can return list of node descriptors and the orchestrator creates new TaskNode instances and adds them to graph (dynamic graph expansion).\n",
    "    - The loop repeats until no ready nodes remain (or max_iterations exceeded).\n",
    "    - Returns a report mapping node_ids to node dicts.\n",
    "\n",
    "    Edge cases handled:\n",
    "    - Missing agent -> node goes error.\n",
    "    - Malformed agent response -> node goes error.\n",
    "    - next_tasks insertion happens at runtime — new nodes become eligible in subsequent iterations.\n",
    "    - There is a max_iterations guard to avoid an infinite loop.\n",
    "\n",
    "    What is not implemented (explicitly):\n",
    "    - Cycle detection (if an agent creates a node that depends on itself or forms a cycle the loop could deadlock or repeatedly attempt — max_iterations helps but is not a proper cycle check).\n",
    "    - Parallel execution — everything runs synchronously in the current implementation.\n",
    "    - Robust persistence/transactionality across restarts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, auth: AuthManager, sessions: SessionManager, memory: MemoryManager):\n",
    "        self.auth = auth\n",
    "        self.sessions = sessions\n",
    "        self.memory = memory\n",
    "        self._agents: Dict[str, Agent] = {}\n",
    "        self._nodes: Dict[str, TaskNode] = {}\n",
    "        # adjacency lists\n",
    "        self._children: Dict[str, List[str]] = defaultdict(list)\n",
    "        self._parents: Dict[str, List[str]] = defaultdict(list)\n",
    "        logger.info(\"GraphOrchestrator initialized\")\n",
    "        print()\n",
    "\n",
    "    # Agent management\n",
    "    def register_agent(self, agent: Agent):\n",
    "        self._agents[agent.agent_id] = agent\n",
    "        logger.info(\"Registered agent %s (%s)\", agent.name, agent.agent_id)\n",
    "        print()\n",
    "\n",
    "    def get_agent(self, agent_id: str) -> Optional[Agent]:\n",
    "        return self._agents.get(agent_id)\n",
    "\n",
    "    # Node/graph management\n",
    "    def add_node(self, node: TaskNode):\n",
    "        if node.node_id in self._nodes:\n",
    "            raise ValueError(\"node_already_exists\")\n",
    "        self._nodes[node.node_id] = node\n",
    "        for p in node.inputs:\n",
    "            self._children[p].append(node.node_id)\n",
    "            self._parents[node.node_id].append(p)\n",
    "        logger.info(\"Added node %s (agent=%s) inputs=%s\", node.node_id, node.agent_id, node.inputs)\n",
    "        print()\n",
    "\n",
    "    def add_edge(self, parent_id: str, child_id: str):\n",
    "        if parent_id not in self._nodes or child_id not in self._nodes:\n",
    "            raise ValueError(\"nodes_must_exist\")\n",
    "        self._children[parent_id].append(child_id)\n",
    "        self._parents[child_id].append(parent_id)\n",
    "\n",
    "    def _ready_nodes(self) -> List[TaskNode]:\n",
    "        # nodes that are pending and whose parents are all done\n",
    "        ready = []\n",
    "        for node in self._nodes.values():\n",
    "            if node.status != \"pending\":\n",
    "                continue\n",
    "            parents = self._parents.get(node.node_id, [])\n",
    "            if all(self._nodes[p].status == \"done\" for p in parents):\n",
    "                ready.append(node)\n",
    "        return ready\n",
    "\n",
    "    def _build_payload_for_node(self, node: TaskNode) -> Dict[str, Any]:\n",
    "        # It takes one argument:\n",
    "        # node — a TaskNode instance that represents the task that’s about to be executed.\n",
    "        # This function's job is to build the JSON payload that will be sent to the agent responsible for executing this node.\n",
    "        # Gather results from parents into payload.inputs[parent_id] = parent.result\n",
    "        # In a task graph, a node may have one or more dependencies (inputs) — \n",
    "        # meaning this node should only run after those parent nodes are done, and it should receive their results.\n",
    "        payload_inputs = {} #This will store the outputs of all parent nodes.\n",
    "        # We loop over each parent node ID listed in the inputs attribute of the current TaskNode\n",
    "        for p in node.inputs: # For each parent p\n",
    "            # We look up that parent node object in the orchestrator’s self._nodes dictionary\n",
    "            # We grab its .result (this is the JSON response returned by the agent that executed that parent task).\n",
    "            # We store it under the parent’s ID in the payload dictionary.\n",
    "            payload_inputs[p] = self._nodes[p].result\n",
    "\n",
    "        #the method returns the final structured payload that will be passed to the agent handling the current node\n",
    "        # \"action\" — what skill or behavior the agent should execute. (e.g., \"process\")\n",
    "        # \"inputs\" — a mapping from parent node IDs to their outputs/results.\n",
    "        # \"metadata\" — the metadata attached to the current node (e.g., parameters, config, context).\n",
    "        return {\"action\": node.action, \"inputs\": payload_inputs, \"metadata\": node.metadata}\n",
    "\n",
    "    def execute_graph(self, session_id: str, max_iterations: int = 1000) -> Dict[str, Any]:\n",
    "        # Attach graph snapshot to session for visibility\n",
    "        session = self.sessions.get_session(session_id)\n",
    "        session_graph = {nid: node.to_dict() for nid, node in self._nodes.items()}\n",
    "        session[\"graph\"] = session_graph\n",
    "\n",
    "        iterations = 0\n",
    "        while True:\n",
    "            if iterations >= max_iterations:\n",
    "                raise RuntimeError(\"max_iterations_reached\")\n",
    "            iterations += 1\n",
    "            ready = self._ready_nodes()\n",
    "            if not ready:\n",
    "                break\n",
    "            for node in ready:\n",
    "                node.status = \"running\"\n",
    "                node.attempts += 1\n",
    "                agent = self.get_agent(node.agent_id)\n",
    "                if not agent:\n",
    "                    node.status = \"error\"\n",
    "                    node.result = {\"status\": \"error\", \"error\": \"missing_agent\"}\n",
    "                    logger.error(\"Node %s has missing agent %s\", node.node_id, node.agent_id)\n",
    "                    print()\n",
    "                    continue\n",
    "                # Create message\n",
    "                msg = Message(frm=session_id, to=node.agent_id, task_id=node.node_id, payload=self._build_payload_for_node(node), metadata={\"node_name\": node.name, \"action\": node.action})\n",
    "                # Call agent\n",
    "                resp = agent.handle_message(msg, session, self.memory)\n",
    "                # Validate\n",
    "                try:\n",
    "                    validate_agent_response(resp)\n",
    "                except MessageValidationError as e:\n",
    "                    node.status = \"error\"\n",
    "                    node.result = {\"status\": \"error\", \"error\": f\"invalid_agent_response:{str(e)}\"}\n",
    "                    logger.error(\"Invalid response from agent for node %s: %s\", node.node_id, e)\n",
    "                    print()\n",
    "                    continue\n",
    "                # Store result\n",
    "                node.result = resp\n",
    "                if resp.get(\"status\") == \"ok\":\n",
    "                    node.status = \"done\"\n",
    "                    # Optionally write memories if response instructs so\n",
    "                    if \"memories\" in resp:\n",
    "                        mems = resp[\"memories\"]\n",
    "                        if isinstance(mems, list):\n",
    "                            for m in mems:\n",
    "                                self.memory.add_short(session_id, m)\n",
    "                                self.memory.add_long(session[\"user_id\"], m)\n",
    "                else:\n",
    "                    node.status = \"error\"\n",
    "                # Handle dynamic next_tasks\n",
    "                next_tasks = resp.get(\"next_tasks\") or []\n",
    "                for nt in next_tasks:\n",
    "                    try:\n",
    "                        # nt expected to be mapping like {\"name\":..., \"agent_id\":..., \"action\":..., \"inputs\": [...]} \n",
    "                        nid = nt.get(\"node_id\") or generate_id(\"dyn\")\n",
    "                        new_node = TaskNode(node_id=nid, name=nt.get(\"name\", nid), agent_id=nt.get(\"agent_id\"), action=nt.get(\"action\"), inputs=nt.get(\"inputs\", []), metadata=nt.get(\"metadata\", {}))\n",
    "                        self.add_node(new_node)\n",
    "                    except Exception:\n",
    "                        logger.exception(\"Failed adding dynamic next_task %s\", nt)\n",
    "                        print()\n",
    "            # continue loop — newly added nodes will be considered in next iteration\n",
    "        # finished; build report\n",
    "        report = {nid: node.to_dict() for nid, node in self._nodes.items()}\n",
    "        logger.info(\"Graph execution finished in %d iterations\", iterations)\n",
    "        print()\n",
    "        return {\"status\": \"ok\", \"report\": report}\n",
    "\n",
    "\n",
    "# ---------- Example / Demo Skills\n",
    "\n",
    "# Skill functions take payload (dict) and ctx (dict) and return a dict response with keys:\n",
    "# - status: 'ok' | 'error'\n",
    "# - result: optional payload\n",
    "# - next_tasks: optional list of nodes to add\n",
    "# - memories: optional list of memory items to store\n",
    "\n",
    "\n",
    "def skill_fetch(payload: Dict[str, Any], ctx: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Demonstration: pretend to fetch data based on query\n",
    "    inputs = payload.get(\"inputs\", {})\n",
    "    query = payload.get(\"metadata\", {}).get(\"query\") or payload.get(\"query\") or \"default query\"\n",
    "    # produce faux data\n",
    "    data = {\"fetched_at\": iso_now(), \"query\": query, \"items\": [f\"item_{i}\" for i in range(3)]}\n",
    "    result = {\"status\": \"ok\", \"result\": data}\n",
    "    # store a short memory and offer a next task 'process'\n",
    "    result[\"memories\"] = [{\"type\": \"fetch\", \"detail\": data}]\n",
    "    # next task instructs the orchestrator to create a processing node that depends on this node\n",
    "    result[\"next_tasks\"] = [\n",
    "        {\"name\": \"process_results\", \"agent_id\": ctx[\"session\"].get(\"metadata\", {}).get(\"processor_agent\", \"agent_processor\"), \"action\": \"process\", \"inputs\": [payload.get(\"metadata\", {}).get(\"origin_node\") or \"unknown_parent\"]}\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "\n",
    "def skill_process(payload: Dict[str, Any], ctx: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Process data from inputs\n",
    "    inputs = payload.get(\"inputs\", {})\n",
    "    # flatten parent results\n",
    "    aggregated = {}\n",
    "    for pid, r in inputs.items():\n",
    "        if not r:\n",
    "            continue\n",
    "        if isinstance(r, dict) and \"result\" in r:\n",
    "            aggregated[pid] = r[\"result\"]\n",
    "        else:\n",
    "            aggregated[pid] = r\n",
    "    # naive processing: count items\n",
    "    stats = {pid: (len(aggregated[pid].get(\"items\", [])) if isinstance(aggregated[pid], dict) else 1) for pid in aggregated}\n",
    "    out = {\"processed_at\": iso_now(), \"stats\": stats, \"summary\": f\"Processed {sum(stats.values())} items\"}\n",
    "    return {\"status\": \"ok\", \"result\": out, \"memories\": [{\"type\": \"process_summary\", \"summary\": out[\"summary\"]}]}\n",
    "\n",
    "\n",
    "def skill_default(payload: Dict[str, Any], ctx: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Generic fallback skill\n",
    "    return {\"status\": \"ok\", \"result\": {\"note\": \"default skill executed\", \"payload\": payload}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3586b-b012-4a6c-857d-fe92429ffa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0f8ee-3b7d-46f1-9a6d-a6458148cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b00072-f729-4532-98c9-d7db6de2d115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33531b-158b-4282-ab67-bfde7e531a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569620e0-5349-4a2b-9eb7-51fd42098075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75e143-8135-4693-8d9b-4afd73310354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Simple test / demo runner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Instantiate managers\n",
    "    auth = AuthManager(token_ttl_seconds=3600)\n",
    "    sessions = SessionManager(session_ttl_seconds=1800)\n",
    "    memory = MemoryManager(short_term_max=100, long_term_path=None)\n",
    "\n",
    "    # Create a user and login\n",
    "    user_id = auth.create_user(\"alice\", \"s3cr3t\")\n",
    "    token = auth.login(\"alice\", \"s3cr3t\")\n",
    "    validated_user = auth.validate_token(token)\n",
    "    print(\"Logged in user id:\", validated_user)\n",
    "\n",
    "    # Create a session tied to user\n",
    "    session_id = sessions.create_session(validated_user, metadata={\"processor_agent\": \"agent_processor\"})\n",
    "    session_obj = sessions.get_session(session_id)\n",
    "\n",
    "    # Create orchestrator and agents\n",
    "    orchestrator = GraphOrchestrator(auth=auth, sessions=sessions, memory=memory)\n",
    "\n",
    "    # Agent that fetches data\n",
    "    a_fetch = Agent(agent_id=\"agent_fetcher\", name=\"Fetcher\")\n",
    "    a_fetch.register_skill(\"fetch\", skill_fetch)\n",
    "    a_fetch.register_skill(\"default\", skill_default)\n",
    "\n",
    "    # Agent that processes data\n",
    "    a_proc = Agent(agent_id=\"agent_processor\", name=\"Processor\")\n",
    "    a_proc.register_skill(\"process\", skill_process)\n",
    "    a_proc.register_skill(\"default\", skill_default)\n",
    "\n",
    "    orchestrator.register_agent(a_fetch)\n",
    "    orchestrator.register_agent(a_proc)\n",
    "\n",
    "    # Build initial nodes\n",
    "    n1 = TaskNode(node_id=\"node_fetch_1\", name=\"fetch_data\", agent_id=\"agent_fetcher\", action=\"fetch\", inputs=[], metadata={\"query\": \"latest reports\", \"origin_node\": \"node_fetch_1\"})\n",
    "    # For demo, we add a processing node that depends on the fetch\n",
    "    n2 = TaskNode(node_id=\"node_process_1\", name=\"process_data\", agent_id=\"agent_processor\", action=\"process\", inputs=[\"node_fetch_1\"], metadata={})\n",
    "\n",
    "    orchestrator.add_node(n1)\n",
    "    orchestrator.add_node(n2)\n",
    "\n",
    "    # Execute graph\n",
    "    result = orchestrator.execute_graph(session_id)\n",
    "\n",
    "    print(\"Execution result (JSON):\\n\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    # Inspect memories\n",
    "    print(\"\\nShort-term memory for session:\\n\", json.dumps(memory.list_short(session_id), indent=2))\n",
    "    print(\"\\nLong-term memory for user:\\n\", json.dumps(memory._long_term.get(user_id, []), indent=2))\n",
    "\n",
    "    # Demonstrate recall\n",
    "    recalled = memory.recall(user_id, keywords=[\"reports\", \"summary\"], session_id=session_id)\n",
    "    print(\"\\nRecall results:\\n\", json.dumps(recalled, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103af5c-7e7a-4f44-a703-3f4d919a5d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f0a0d-ad3c-47cb-90b8-7d0b7147e050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73883f-e9a2-4753-b554-cf977bfe2244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33481d-2e00-4e2b-8141-ddb98bc2b946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529149f-c06c-4473-893f-aab7ffc609bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98752e8-8378-4912-bfb7-0ba72dc0ad14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd45f0-4d95-49ab-9eee-5465bf843e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74de64-5d7c-466c-984e-c41d355f9ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c24263-792d-43b5-b1ef-1e98bfea8051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a2e40-5f7a-483a-bcc6-e8f255da94a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96dc0c-987c-4322-a5c5-f5fcf9592f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2e902-7e5f-4c98-b650-fea3f8f505d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc7085-2c5d-475c-9c99-b12360e55cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61520c50-bf91-4cba-ac4e-1e45872dc37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02699fbe-000f-4d19-8bef-373e8c52a74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e69a9e-b47b-47f9-8f37-9df7dc5f36d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21886e7-4ce6-4778-97d1-2700c732d557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14619c19-a0d9-4a9e-a1fb-335740185919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36da090-905c-45e2-8a6b-c8bc99a915e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9915d9f-2b92-4266-9eb1-6bd1b7e95e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f2fe2-031b-496d-9bef-3b24fba707a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef381b-ef3e-4cb2-85d2-20a278a2c1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47476c-e164-46b5-bcee-d88a2a08a226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
